{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "535f16b66d12363ace62e46632df2fd7",
     "grade": false,
     "grade_id": "cell-527cc2929f6c6660",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### HW 4: Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MICHAEL MEROLA, mime4339\n",
    "\n",
    "# COLLABORATORS\n",
    "# Krish Dholakiya, Amari Hoogland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2870fc92e6d85aa81b586a60ec54b2d",
     "grade": false,
     "grade_id": "cell-ff2d107bf94e97a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Part 1 - Theory / Written Problems\n",
    "\n",
    "\n",
    "### [16 points] Problem 1 - Naive Bayes for Tennis Prediction \n",
    "***\n",
    "\n",
    "Suppose you are trying to learn a person's decision whether to play tennis or not on a given day using features corresponding to precipitation forecast, temperature, humidity, and wind. You're given the following training data: \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c|c}\n",
    "\\textbf{Forecast} & \\textbf{Temp} & \\textbf{Humidity} & \\textbf{Wind} & \\textbf{PlayTennis} \\\\\n",
    "\\hline \n",
    "\\textrm{sunny} & \\textrm{hot} & \\textrm{high} & \\textrm{weak} & \\textbf{No} \\\\ \n",
    "\\textrm{sunny} & \\textrm{hot} & \\textrm{high} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\textrm{overcast} & \\textrm{hot} & \\textrm{high} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{high} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{cool} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{cool} & \\textrm{normal} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\textrm{overcast} & \\textrm{cool} & \\textrm{normal} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{sunny} & \\textrm{mild} & \\textrm{high} & \\textrm{weak} & \\textbf{No} \\\\ \n",
    "\\textrm{sunny} & \\textrm{cool} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{sunny} & \\textrm{mild} & \\textrm{normal} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{overcast} & \\textrm{mild} & \\textrm{high} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{overcast} & \\textrm{hot} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{high} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Part A (2pt)**: Estimate the **unconditional** probabilities $p(\\textrm{PlayTennis=Yes})$ and $p(\\textrm{PlayTennis=No})$ from the training data. These are also called the \"prior\" probabilities. **Note in all the problems that follow, unless directed otherwise, you must show/describe how you arrived at your answer, even if the method is simple. Simply writing  the numeric answer is not enough.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6992779ca5b743987a6bd7324c0ef479",
     "grade": true,
     "grade_id": "cell-e1f2324117ff87fd",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$p(\\textrm{PlayTennis=Yes})$ = # of 'Yes' / # Total = 9/14\n",
    "\n",
    "$p(\\textrm{PlayTennis=No})$ = # of 'No' / # Total = 5/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f61741ce49ab061d0755f6857c2fe76",
     "grade": false,
     "grade_id": "cell-a2c17a6972949b0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B (6 pt)**: \n",
    "1. For each feature, estimate the class-conditional probabilities $p(\\textrm{feature value} \\mid \\textrm{PlayTennis} = \"Yes\" )$ and $p(\\textrm{feature value} \\mid \\textrm{PlayTennis} = \"No\" )$. Show your work.\n",
    "\n",
    "2. For a given probability mass function for a discrete random variable $X$, the sum of the probabilities over the range of values the variable can take must add up to 1. Is the following equation true? Why or why not?\n",
    "\n",
    "$$  p(\\textrm{Forecast=Sunny} | \\textrm{Humidity = high} ) +  p(\\textrm{Forecast=Overcast} | \\textrm{Humidity = low} ) +  p(\\textrm{Forecast=rainy} | \\textrm{Humidity = normal} ) = 1 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c02c0014bb4493b3d07db85039468be",
     "grade": true,
     "grade_id": "cell-2584fb81679aaf31",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "    1.\n",
    "\n",
    "$p(\\textrm{Forecast = \"sunny\"} \\mid \\textrm{PlayTennis} = \"Yes\" )$ = 2/9\n",
    "\n",
    "$p(\\textrm{Forecast = \"sunny\"} \\mid \\textrm{PlayTennis} = \"No\" )$ = 3/5\n",
    "\n",
    "$p(\\textrm{Forecast = \"overcast\"} \\mid \\textrm{PlayTennis} = \"Yes\" )$ = 4/9\n",
    "\n",
    "$p(\\textrm{Forecast = \"overcast\"} \\mid \\textrm{PlayTennis} = \"No\" )$ = 0/5\n",
    "\n",
    "$p(\\textrm{Forecast = \"rainy\"} \\mid \\textrm{PlayTennis} = \"Yes\" )$ = 3/9\n",
    "\n",
    "$p(\\textrm{Forecast = \"rainy\"} \\mid \\textrm{PlayTennis} = \"No\" )$ = 2/5\n",
    "\n",
    "$p(\\textrm{Temp = \"hot\"} \\mid \\textrm{PlayTennis} = \"Yes\" )$ = 2/9\n",
    "\n",
    "$p(\\textrm{Temp = \"hot\"} \\mid \\textrm{PlayTennis} = \"No\" )$ = 2/5\n",
    "\n",
    "$p(\\textrm{Temp = \"mild\"} \\mid \\textrm{PlayTennis} = \"Yes\" )$ = 4/9\n",
    "\n",
    "$p(\\textrm{Temp = \"mild\"} \\mid \\textrm{PlayTennis} = \"No\" )$ = 2/5\n",
    "\n",
    "$p(\\textrm{Temp = \"cool\"} \\mid \\textrm{PlayTennis} = \"Yes\" )$ = 3/9\n",
    "\n",
    "$p(\\textrm{Temp = \"cool\"} \\mid \\textrm{PlayTennis} = \"No\" )$ = 1/5\n",
    "\n",
    "$p(\\textrm{Humidity = \"high\"} \\mid \\textrm{PlayTennis} = \"Yes\" )$ = 3/9\n",
    "\n",
    "$p(\\textrm{Humidity = \"high\"} \\mid \\textrm{PlayTennis} = \"No\" )$ = 4/5\n",
    "\n",
    "$p(\\textrm{Humidity = \"normal\"} \\mid \\textrm{PlayTennis} = \"Yes\" )$ = 6/9\n",
    "\n",
    "$p(\\textrm{Humidity = \"normal\"} \\mid \\textrm{PlayTennis} = \"No\" )$ = 1/5\n",
    "\n",
    "$p(\\textrm{Wind = \"weak\"} \\mid \\textrm{PlayTennis} = \"Yes\" )$ = 6/9\n",
    "\n",
    "$p(\\textrm{Wind = \"weak\"} \\mid \\textrm{PlayTennis} = \"No\" )$ = 2/5\n",
    "\n",
    "$p(\\textrm{Wind = \"strong\"} \\mid \\textrm{PlayTennis} = \"Yes\" )$ = 3/9\n",
    "\n",
    "$p(\\textrm{Wind = \"strong\"} \\mid \\textrm{PlayTennis} = \"No\" )$ = 3/5\n",
    "\n",
    "    2.\n",
    "    \n",
    "The equation is false because each condition is different from the other, so theres a possibility it could add up to less than or greater than 1. An example of the correct equation would be:\n",
    "\n",
    "$$  p(\\textrm{Forecast=Sunny} | \\textrm{Humidity = high} ) +  p(\\textrm{Forecast=Overcast} | \\textrm{Humidity = high} ) +  p(\\textrm{Forecast=rainy} | \\textrm{Humidity = high} ) = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e0315f64323611f7e176c184de3a3ef",
     "grade": false,
     "grade_id": "cell-dc5403343700c01f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part C (4 pt)**:\n",
    "    \n",
    "1. Consider the Naive Bayes model where the $x_i$ features are discrete random variables (also known as Multinomial Naive Bayes): \n",
    "\n",
    "$$ p(y \\mid x_1, ..., x_n) \\approx p(y) \\Pi_i^n p(x_i | y) $$\n",
    "\n",
    "In this case, our prediction is made by:\n",
    "\n",
    "$$ \\hat{y}(x) = \\mathop{\\mathrm{argmax}}_y p(y) \\Pi_i^n p(x_i \\mid y) $$\n",
    "\n",
    "What will happen if we receive a query point $x$ such that the $i$-th feature $x_i$ contains a value not in the original dataset. Suppose this value is $v$. What is $p(x_i=v \\mid y)$? Such a case may occur if, for example, our features are words in a sentance. Then a rare word may appear that was not in the original training dataset. Assume you trained your model only on words in the training data using exact counts of features to estimate the requisuite probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7eb90b9c15f02f23d066d3317ec07802",
     "grade": true,
     "grade_id": "cell-1c0e5ee75b537eef",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "In this case, the $p(x_i=v \\mid y)$ would be equal to 0 because there is no count associated with $p(x_i=v)$ from the training set. This would result in an incorrect probability because the resulting calculation would not correctly represent that feature in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D (4 pts)**: \n",
    "One way to solve the problem you discovered in (1) is to artificially inflate the count of all possible words by some small number. So in an NLP dataset, all possible words would have at least some minimum count when calculating the above model. We can also introduce an `<unk>` word with a minimum count that is assigned to any words that we didn't account for at train-time. Inflating the counts results in so-called **Laplace Smoothing**:\n",
    "\n",
    "$$p(x_i =j| y_i=y) \\approx \\frac{N_{yij}+ \\alpha }{N_y + \\alpha D} $$\n",
    "\n",
    "where $N_{y}$ is the total count of all samples with label $y$, $N_{yij}$ is the total count of all samples such that $x_i=j$ and $y_i=y$, and $D$ is the number of features. $\\alpha$ is an adjustable parameter and can be thought of as a an additional factor added to each feature count, although it need not be an integer.\n",
    "\n",
    "Assume a Naive Bayes model with Laplace Smoothing such that $\\alpha=2$. What would your Naive Bayes model predict for the following two weather conditions? Show all work.  \n",
    "- **Forecast**=overcast, **Temp**=cool, **Humidity**=high, **Wind**=weak  \n",
    "- **Forecast**=sunny, **Temp**=hot, **Humidity**=normal, **Wind**=strong  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32bf31ed81bed731f087d4980b964d89",
     "grade": true,
     "grade_id": "cell-762f9fd576f071b9",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "    1.\n",
    "\n",
    "$X = $[ **Forecast**=overcast, **Temp**=cool, **Humidity**=high, **Wind**=weak ]\n",
    "\n",
    "$p(PlayTennis = Yes \\mid X) = {p(X \\mid PlayTennis = Yes) * p(PlayTennis = Yes) \\over p(X)}$ \n",
    "\n",
    "$p(X \\mid PlayTennis = Yes) = p(Forecast = overcast \\mid PlayTennis = Yes) * p(Temp = cool \\mid PlayTennis = Yes) * p(Humidity = high \\mid PlayTennis = Yes) * p(Wind = weak \\mid PlayTennis = Yes) = 4/9 * 3/9 * 3/9 * 6/9 => (laplace) => 6/17 * 5/17 * 5/17 * 8/17 = 0.0143 $\n",
    "\n",
    "$p(PlayTennis = Yes) = 9/14 => (laplace) => 11/22 = 0.5 $\n",
    "\n",
    "$p(X) = p(Forecast=overcast) * p(Temp=cool) * p(Humidity=high) * p(Wind=weak) = 4/14 * 4/14 * 7/14 * 8/14 => (laplace) => 6/22 * 6/22 * 9/22 * 10/22 = 0.0138 $\n",
    "\n",
    "$p(PlayTennis = Yes \\mid X) = {0.0143 * 0.5 \\over 0.0138} = 0.497 = $ *Model predicts that we will NOT play tennis*\n",
    "\n",
    "    2.\n",
    "    \n",
    "$Y = $[ **Forecast**=sunny, **Temp**=hot, **Humidity**=normal, **Wind**=strong ]\n",
    "\n",
    "$p(PlayTennis = Yes \\mid Y) = {p(Y \\mid PlayTennis = Yes) * p(PlayTennis = Yes) \\over p(Y)}$ \n",
    "\n",
    "$p(Y \\mid PlayTennis = Yes) = p(Forecast = sunny \\mid PlayTennis = Yes) * p(Temp = hot \\mid PlayTennis = Yes) * p(Humidity = normal \\mid PlayTennis = Yes) * p(Wind = strong \\mid PlayTennis = Yes) = 2/9 * 2/9 * 6/9 * 3/9 => (laplace) => 4/17 * 4/17 * 8/17 * 5/17 = 0.007 $\n",
    "\n",
    "$p(PlayTennis = Yes) = 9/14 => (laplace) => 11/22 = 0.5 $\n",
    "\n",
    "$p(Y) = p(Forecast=sunny) * p(Temp=hot) * p(Humidity=normal) * p(Wind=strong) = 5/14 * 4/14 * 7/14 * 6/14 => (laplace) => 7/22 * 6/22 * 9/22 * 8/22 = 0.013 $\n",
    "\n",
    "$p(PlayTennis = Yes \\mid Y) = {0.007 * 0.5 \\over 0.013} = 0.27 = $ *Model predicts that we will NOT play tennis*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "244fe72bfacc1d7a4612322e9a2f28fa",
     "grade": false,
     "grade_id": "cell-298b5a356f5fd701",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### PART 2 [15 points] Implementing Naive Bayes for Text Classification \n",
    "***\n",
    "\n",
    "In this problem you'll implement a general Discrete Naive Bayes class for text classification. Your tasks will be to implement `train`, `predict_log_score`, and `predict` routines to learn the Naive Bayes model parameters from a collection on text and make predictions on unseen validation data. \n",
    "\n",
    "The skeleton for the `TextNB` class is below. \n",
    "\n",
    "**Important Note**: In Problem 3 we'll be using the `TextNB` class to make predictions about Twitter data.  Since real-world text data typically has a large number of features, you'll want to make your implementation reasonably efficient so that your experiments in Part 3 don't take forever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b918713fe93e088de73222c7b5c0f342",
     "grade": false,
     "grade_id": "cell-81e29b259b0173d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TextNB:\n",
    "    def __init__(self, text_train, y_train, alpha=1.0):\n",
    "        \"\"\"\n",
    "        :param text_train: a list or ndarray of text strings to use as training data \n",
    "        :param y_train: an ndarray of true labels associated with the text data \n",
    "        :param alpha: the Laplace smoothing parameter \n",
    "        \"\"\"\n",
    "        \n",
    "        # store training data \n",
    "        self.text_train = text_train \n",
    "        self.y_train = y_train \n",
    "        \n",
    "        # store smoothing parameter\n",
    "        self.alpha = alpha \n",
    "        \n",
    "        # get number of classes \n",
    "        self.num_classes = len(np.unique(y_train))\n",
    "        \n",
    "        # initialize vocab to feature map \n",
    "        self.vocab = dict() \n",
    "        \n",
    "        # initialize class counts \n",
    "        self.class_counts = np.zeros(self.num_classes, dtype=int)\n",
    "        \n",
    "        # initialize feature counts (Note, will need to update this with the correct\n",
    "        # number of columns during the training process)\n",
    "        self.feature_counts = np.zeros((self.num_classes, 0), dtype=int)\n",
    "        \n",
    "        # Any extra fields you want go here\n",
    "        # YOUR CODE HERE\n",
    "        self.totalwords = 0\n",
    "        self.classwords = np.zeros(self.num_classes, dtype=int)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Learn the vocabularly, class_counts, and feature counts from the training data \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        #convert inputs to np arrays for consistency\n",
    "        if not isinstance(self.text_train,np.ndarray): \n",
    "            self.text_train=np.array(self.text_train)\n",
    "            \n",
    "        if not isinstance(self.y_train,np.ndarray): \n",
    "            self.y_train=np.array(self.y_train)\n",
    "        \n",
    "        \n",
    "        #train on text\n",
    "        i = 0 \n",
    "        for sentence in self.text_train:\n",
    "            words = sentence.split()\n",
    "            \n",
    "            for w in words:\n",
    "                self.totalwords += 1\n",
    "                if w not in self.vocab: \n",
    "                    self.vocab[w] = i\n",
    "                    i += 1\n",
    "        \n",
    "        #update labels\n",
    "        for label in self.y_train:\n",
    "            self.class_counts[label] += 1\n",
    "        \n",
    "        self.feature_counts = np.zeros((self.num_classes, len(self.vocab.keys()))) ####\n",
    "        \n",
    "        #update features\n",
    "        for count, sentence in enumerate(self.text_train): \n",
    "            j = self.y_train[count]\n",
    "            words = sentence.split()\n",
    "            \n",
    "            for w in words:\n",
    "                loc = self.vocab[w]\n",
    "                self.feature_counts[j][loc] += 1\n",
    "                self.classwords[self.y_train[count]] += 1\n",
    "                \n",
    "                \n",
    "            count += 1       \n",
    "                    \n",
    "                    \n",
    "    def predict_log_score(self, text_str):\n",
    "        \"\"\"\n",
    "        Get the log-probability score for each class\n",
    "        for a query string\n",
    "        \n",
    "        :param text_str: a single string of text to compute the log_score for \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        words = text_str.split()\n",
    "        c_prob = np.array([])\n",
    "        \n",
    "        for c in range(self.num_classes):\n",
    "            p = np.array([])\n",
    "            \n",
    "            for w in words:\n",
    "                if w in self.vocab.keys():\n",
    "                    idx = self.vocab[w]\n",
    "                    p = np.append(p, np.log((self.feature_counts[c][idx]+self.alpha)/\n",
    "                                    (self.classwords + len(self.vocab.keys())*self.alpha))\n",
    "                            )\n",
    "            classpred = (sum(p) + np.log((self.class_counts[c] + self.alpha)))/(self.totalwords + (self.num_classes * self.alpha))\n",
    "                         \n",
    "            c_prob = np.append(c_prob, classpred)\n",
    "            #print(len(c_prob))\n",
    "        \n",
    "        return c_prob\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self, text_list):\n",
    "        \"\"\"\n",
    "        Predict the class of each example in text_list  \n",
    "        \n",
    "        :param text_list: a list or ndarray of text strings to make predictions on \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        labels = np.zeros(len(text_list), dtype=int)\n",
    "\n",
    "        for count, w in enumerate(text_list):\n",
    "            logs = self.predict_log_score(w)\n",
    "            labels[count] = max(logs)\n",
    "\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d262235a4f20151388dba619ac247a24",
     "grade": false,
     "grade_id": "cell-00ea243e102e6fa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Not that in all of the following you may NOT use scikit-learn NaiveBayes classes.**\n",
    "\n",
    "**Part A**: Complete the `train` function in the `TextNB` class to prepare to make Naive Bayes predictions using Laplace smoothing.  In this routine you will need to populate the following data structures: \n",
    "\n",
    "`self.vocab`: A Python dictionary that maps distinct terms found in the training set to unique indices in $\\{0, 1, \\ldots, |V|-1\\}$.  This will allow us to quickly look up frequency counts for an encountered term in a Numpy array. Note that while the data is fairly clean (We've removed punctuation and made all characters lowercase) you should take care that you're not accidentally counting whitespace in the vocabulary. \n",
    "\n",
    "`self.class_counts`: A 1D Numpy array of length `self.num_classes` which counts the number of documents in the training set that belong to each class. \n",
    "\n",
    "`self.feature_counts`: A 2D Numpy array of dimensions `self.num_classes` $\\times$ $|V|$. The $(c,k)$-entry in this array should be the number of times that term $k$ appears in documents belonging to class $c$. Note that we're using the Bag-of-Words approach here, so if a term appears multiple times in a single document, each instance of that term should be counted.  \n",
    "\n",
    "When you think you're done, execute the following unit test. 2 are available for you, one is hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51af6e50bbd2f6d58dcbcc6227117e62",
     "grade": true,
     "grade_id": "cell-e925aff87ac8cf0b",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def testVocab(): \n",
    "        \"\"\"\n",
    "        test vocabulary \n",
    "        \"\"\"\n",
    "        text_train = np.array([\"work buy  money\", \"prince opportunity viagra\", \"fly   buy prince\", \"money buy fly\", \"fly home prince\"])\n",
    "        y_train = np.array([0, 1, 1, 1, 0], dtype=int)\n",
    "        nb = TextNB(text_train, y_train)\n",
    "        nb.train()\n",
    "        vocab_set = set([\"work\", \"buy\", \"money\", \"prince\", \"opportunity\", \"viagra\", \"fly\", \"home\"])\n",
    "        for w in vocab_set:\n",
    "            assert(w in nb.vocab)\n",
    "            assert(nb.vocab[w] in set(list(range(8))))\n",
    "            \n",
    "def testClassCounts(): \n",
    "        \"\"\"\n",
    "        test counts of documents in each class \n",
    "        \"\"\"\n",
    "        text_train = np.array([\"work buy  money\", \"prince opportunity viagra\", \"fly   buy prince\", \"money buy fly\", \"fly home prince\"])\n",
    "        y_train = np.array([0, 1, 1, 1, 0], dtype=int)\n",
    "        nb = TextNB(text_train, y_train)\n",
    "        nb.train()\n",
    "        vocab_set = set([\"work\", \"buy\", \"money\", \"prince\", \"opportunity\", \"viagra\", \"fly\", \"home\"])\n",
    "        assert(nb.class_counts[0] == 2)\n",
    "        assert(nb.class_counts[1] == 3)\n",
    "\n",
    "testVocab()\n",
    "testClassCounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c38783a5391c18d6d842fd0a042ddb6f",
     "grade": true,
     "grade_id": "cell-acbeb3b7ea9f7c20",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70100d4460002dc59db46c9581a58438",
     "grade": false,
     "grade_id": "cell-a70e28244937c0e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B**: Complete the `predict_log_score` function in the `TextNB` class to take in a single string of text and compute the associated log-score for each class. For now, you should use Laplace smoothing with $\\alpha=1$ for both the class-priors and the class-conditional probabilities.  In **Problem 3** we'll experiment with different variants of Laplace smoothing, so if you like you can read ahead now and implement the general version of Laplace smoothing from the beginning.  \n",
    "\n",
    "**Note**: For simplicity and testing purposes, do not implement an `UNK` feature.  Instead, if you encounter a term not in the vocabulary you can safely ignore it. \n",
    "\n",
    "When you think your `predict_log_score` function is working well, execute the following unit test. Some unit tests are hidden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb711c11d8c77ff238a75c4a1284150b",
     "grade": true,
     "grade_id": "cell-58f78a77269133ab",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.65643039 -0.63950792] [-0.65643039 -0.54410436]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-258-ac820e36dbb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtestPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-258-ac820e36dbb9>\u001b[0m in \u001b[0;36mtestPrediction\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6.685\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m6.9798\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6.685\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m6.1689\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def testPrediction(): \n",
    "    \"\"\"\n",
    "    test vocabulary \n",
    "    \"\"\"\n",
    "    text_train = np.array([\"work buy  money\", \"prince opportunity viagra\", \"fly   buy prince\", \"money buy fly\", \"fly home prince\"])\n",
    "    y_train = np.array([0, 1, 1, 1, 0], dtype=int)\n",
    "    nb = TextNB(text_train, y_train, alpha=1.0)\n",
    "    nb.train()\n",
    "    test1 = \"money money money\"\n",
    "    test2 = \"money fly buy\"\n",
    "    logits1 = nb.predict_log_score(test1)\n",
    "    logits2 = nb.predict_log_score(test2)\n",
    "    print(logits1, logits2)\n",
    "    diff = np.abs(logits1-np.array([-6.685, -6.9798]))\n",
    "    assert(diff.sum()<1e-4)\n",
    "    diff = np.abs(logits2-np.array([-6.685, -6.1689]))\n",
    "    assert(diff.sum()<1e-4)\n",
    "    \n",
    "testPrediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d68869e8a37c042445265d45be8ce12",
     "grade": false,
     "grade_id": "cell-e32a645bef8f4e0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part C**: Finally, implement the `predict` method to take in a list or ndarray of text data, call `predict_log_score`, and return a vector of predicted labels. \n",
    "\n",
    "When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eda99cc7fdb8db00f5542a5744261b1c",
     "grade": true,
     "grade_id": "cell-436849047d2409a9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-2bbcba55d60c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myh\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0myi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtestPredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-259-2bbcba55d60c>\u001b[0m in \u001b[0;36mtestPredict\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myh\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0myi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtestPredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def testPredict(): \n",
    "    text_train = np.array([\"work buy  money\", \"prince opportunity viagra\", \"fly   buy prince\", \"money buy fly\", \"fly home prince\"])\n",
    "    y_train = np.array([0, 1, 1, 1, 0], dtype=int)\n",
    "    nb = TextNB(text_train, y_train)\n",
    "    nb.train() \n",
    "    yhat = nb.predict(text_train)\n",
    "    assert(len(yhat)== 5)\n",
    "    assert(all(yh==yi for yh, yi in zip(yhat, y_train)))\n",
    "        \n",
    "testPredict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf01357f9b33cc67a2269e559026fdcd",
     "grade": false,
     "grade_id": "cell-cb39da44cc9b3251",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [12 points] Problem 3: Predicting the Sentiment of Tweets sent from Passengers to Airlines \n",
    "***\n",
    "\n",
    "In this problem you'll use the `TextNB` class you wrote in **Problem 2** to make predictions about the sentiment of tweets sent by passengers to airlines.  Execute the following cell to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path='airline_tweets.pklz'):\n",
    "    f = open('airline_tweets.pklz','rb')\n",
    "    text_train, y_train, text_valid, y_valid, text_all, y_all = np.load(f, allow_pickle=True)\n",
    "    f.close()\n",
    "    return {\n",
    "        'train': {\n",
    "            'tweets': text_train,\n",
    "            'labels': y_train\n",
    "        },\n",
    "        'valid': {\n",
    "            'tweets': text_valid,\n",
    "            'labels': y_valid\n",
    "        },\n",
    "        'all': {\n",
    "            'tweets': text_all,\n",
    "            'labels': y_all\n",
    "        }\n",
    "    }\n",
    "\n",
    "# This should be the only global variable you use in your cells.\n",
    "# Treat it as immutable. Do not access it directly in any function, it only gets\n",
    "# passed in.\n",
    "global_dataset = get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a9aeb8a7719d4f6c3dcf164b097ef15",
     "grade": false,
     "grade_id": "cell-53c119a1daedc563",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part A (4Pts)**: Explore the data and fill in the below function toanswer the following questions: \n",
    "\n",
    "- How many total examples are there in the training set?\n",
    "- How many total examples are there in the validation set?\n",
    "- Which binary label ($\\{0,1\\}$) corresponds to tweets with positive sentiment?\n",
    "- What percentage of tweets in the training set have true positive sentiment? \n",
    "- What percentage of tweets in the validation set have true positive sentiment? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell is for scratch space to explore the data before filling in the below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e3e476489db60db516ed3b70017a2be",
     "grade": false,
     "grade_id": "cell-1a9a6c6a18b5e01b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@southwestair awesome  thanks'\n",
      " '@usairways good news weve located the crew and made contact with them flight was supposed to leave 4 minutes ago usairwaysfail'\n",
      " '@usairways 45 minute delay for take off and 30 minute wait for checked bags really'\n",
      " ...\n",
      " '@southwestair will continue to be my airline of choice @united most frustrating travel day ive experienced 5 delays and 4 gate changes'\n",
      " '@americanair would it be ok to send you a dm asking a few questions because im and been on hold so long'\n",
      " '@united this is it last time i fly unitedairlines  you screw up every trip now will be stuck in ord and miss work']\n",
      "4000\n",
      "[1 0 0 ... 0 0 0]\n",
      "4000\n",
      "['@southwestair swa agent @orangecounty tells me youre not going 2 today or back 2 phx flew @usairways'\n",
      " '@united done just now thanks'\n",
      " '@united weve had a of problems with getting our bags and have been given the run around for a day whats up with that'\n",
      " ... '@usairways  you make the reservation well make the usairwaysfail'\n",
      " '@united can you you think about some baggage attendants at ewr been waiting for bags for 1 hr united '\n",
      " '@usairways i have nothing but issues when i fly whats up travel vacation awful']\n",
      "4000\n",
      "train percent pos =  0.25\n",
      "train percent pos =  0.23075\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_info(dataset):\n",
    "    '''\n",
    "    dataset - the loaded dataset\n",
    "    \n",
    "    returns:\n",
    "        np.array of size 5 with the elements from above (in order)\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    trainingtweets = dataset['train']['tweets']\n",
    "    traininglabels = dataset['train']['labels']\n",
    "    print(trainingtweets)\n",
    "    print(len(trainingtweets))\n",
    "    print(traininglabels)\n",
    "    print(len(traininglabels))\n",
    "    \n",
    "    validtweets = dataset['valid']['tweets']\n",
    "    validlabels = dataset['valid']['labels']\n",
    "    print(validtweets)\n",
    "    print(len(validtweets))\n",
    "    \n",
    "    countp = 0\n",
    "    for i, t in enumerate(trainingtweets):\n",
    "        if traininglabels[i] == 1:\n",
    "            countp += 1\n",
    "            \n",
    "    print(\"train percent pos = \", countp/len(traininglabels))\n",
    "    \n",
    "    countpv = 0\n",
    "    for i, t in enumerate(validtweets):\n",
    "        if validlabels[i] == 1:\n",
    "            countpv += 1\n",
    "            \n",
    "    print(\"train percent pos = \", countpv/len(validlabels))\n",
    "    \n",
    "    \n",
    "get_dataset_info(get_dataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Set = 4000 tweets\n",
    "\n",
    "validation Set = 4000 tweets\n",
    "\n",
    "0 label = negative\n",
    "\n",
    "1 label = positive\n",
    "\n",
    "percent positive tweets in training = 25%\n",
    "\n",
    "percent positive tweets in valid = 23%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "762cd3e4334e4e9f87ce5e74a96f5389",
     "grade": true,
     "grade_id": "cell-44d6f1137fdafd2f",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE OR DELETE THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f12b5f0b5ebe80820a08bfc96f1ee17",
     "grade": false,
     "grade_id": "cell-b9ff55fe37bb90ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B (4 Pts)**: Use your `TextNB` class to learn a Naive Bayes classifier for the airline Twitter data.  What accuracy do you achieve on the training set and what accuracy, precision, and recall do you achieve on the validation set? \n",
    "\n",
    "You may use `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ed382124d0b35510aa0d0c5675315cf",
     "grade": false,
     "grade_id": "cell-f5c5782edb8ff5d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc 0.85425\n",
      "train prec 0.6378056840713814\n",
      "train rec 0.965\n",
      "valid acc 0.83375\n",
      "valid prec 0.5871621621621622\n",
      "valid rec 0.9414951245937161\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def build_and_evaluate(dataset, alpha=1.0):\n",
    "    '''\n",
    "    args - \n",
    "        dataset: the dataset object\n",
    "        alpha: the alpha value (for Laplace smoothing) given to TextNB\n",
    "    \n",
    "    Returns an np.array of 6 numbers: \n",
    "        train set accuracy,\n",
    "        train set precision, \n",
    "        train set recall, \n",
    "        valid set accuracy, \n",
    "        valid set precision,\n",
    "        valid set recall\n",
    "        \n",
    "    You should use your TextNB class\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    baes = TextNB(dataset['train']['tweets'], dataset['train']['labels'], alpha)\n",
    "    baes.train()\n",
    "    \n",
    "    trainPrediction = baes.predict(dataset['train']['tweets'])\n",
    "    validPrediction = baes.predict(dataset['valid']['tweets'])\n",
    "    \n",
    "    return np.array([\n",
    "        metrics.accuracy_score(dataset['train']['labels'], trainPrediction),\n",
    "        metrics.precision_score(dataset['train']['labels'], trainPrediction),\n",
    "        metrics.recall_score(dataset['train']['labels'], trainPrediction),\n",
    "        metrics.accuracy_score(dataset['valid']['labels'], validPrediction),\n",
    "        metrics.precision_score(dataset['valid']['labels'], validPrediction),\n",
    "        metrics.recall_score(dataset['valid']['labels'], validPrediction)]\n",
    "        )\n",
    "\n",
    "model = build_and_evaluate(get_dataset(), 1.0)\n",
    "\n",
    "print(\"train acc\", model[0])\n",
    "print(\"train prec\", model[1])\n",
    "print(\"train rec\", model[2])\n",
    "print(\"valid acc\", model[3])\n",
    "print(\"valid prec\", model[4])\n",
    "print(\"valid rec\", model[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0273605aedb3532c8cc184ebf20b3f5",
     "grade": true,
     "grade_id": "cell-bb7c76ec22048a6f",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE OR DELETE THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e59018303579cfea8501262840ea7f2",
     "grade": false,
     "grade_id": "cell-1593aa1b6d1c82b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part D**: Write a procedure to perform $K$-Folds cross-validation on the entire data set (`train_all` and `y_all`) to estimate the accuracy of your NB classifier for various values of $\\alpha$ and make a plot showing your results.  \n",
    "\n",
    "To do the partitioning into folds we recommend leveraging sklearn's [StratifiedKFold](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-with-stratification-based-on-class-labels) routine.  The documentation demonstrates how it can be used.  \n",
    "\n",
    "For your plot, use at least $K=5$ folds and at least $5$ different values of $\\alpha$ between $0.1$ and $1.5$ (or higher).  Which value of $\\alpha$ seems to perform the best? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f4f6ce8423693b3f5cd2a0c4d374ab7",
     "grade": true,
     "grade_id": "cell-7ef6a941fdcb5f81",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def kfold_validation(dataset, alpha_vals=np.linspace(0.1, 1.5, 5)):\n",
    "    '''\n",
    "    Use your build_and_evaluate function to perform kfold_validation\n",
    "    to select the alpha parameter.\n",
    "    \n",
    "    You should return a A x 6 array, 1 row for each alpha value,\n",
    "    and 1 column for each metric returned from build_and_evaluate\n",
    "    '''\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "706f4b4658f0d29990a475230357fc48",
     "grade": true,
     "grade_id": "cell-1bbf1c7c45a697ea",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3d656679c97837202a4f6ae734e7dc6",
     "grade": true,
     "grade_id": "cell-69661fdbdfd6efcb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not modify this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
